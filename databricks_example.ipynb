{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 1: Install from git repository\n",
        "%pip install git+https://github.com/yourusername/petrinex-python-api.git --quiet\n",
        "\n",
        "# Option 2: Install from local wheel (if uploaded to DBFS)\n",
        "# %pip install /dbfs/FileStore/wheels/petrinex-0.1.0-py3-none-any.whl --quiet\n",
        "\n",
        "# Restart Python kernel to use the newly installed package\n",
        "dbutils.library.restartPython()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from petrinex import PetrinexVolumetricsClient\n",
        "from pyspark.sql import functions as F\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Initialize the Petrinex client\n",
        "client = PetrinexVolumetricsClient(\n",
        "    spark=spark,\n",
        "    jurisdiction=\"AB\",      # Alberta\n",
        "    file_format=\"CSV\"\n",
        ")\n",
        "\n",
        "print(\"âœ“ Petrinex client initialized successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " copy the "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check what files have been updated in the last 30 days\n",
        "cutoff_date = (datetime.now() - timedelta(days=30)).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "files = client.list_updated_after(cutoff_date)\n",
        "\n",
        "print(f\"Found {len(files)} file(s) updated after {cutoff_date}\\n\")\n",
        "print(\"Production Month | Updated Date        | URL\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "for f in files[:10]:  # Show first 10\n",
        "    print(f\"{f.production_month:15} | {str(f.updated_ts):19} | {f.url}\")\n",
        "\n",
        "if len(files) > 10:\n",
        "    print(f\"\\n... and {len(files) - 10} more files\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the cutoff date (e.g., load data updated in 2026)\n",
        "updated_after = \"2026-01-01\"\n",
        "\n",
        "print(f\"Loading data updated after {updated_after}...\")\n",
        "print(\"This may take a few minutes depending on the number of files...\\n\")\n",
        "\n",
        "# Read data using the pandas-based method (UC-friendly)\n",
        "df = client.read_updated_after_as_spark_df_via_pandas(\n",
        "    updated_after,\n",
        "    pandas_read_kwargs={\n",
        "        \"dtype\": str,           # Force all columns to string (avoid mixed types)\n",
        "        \"encoding\": \"latin1\"    # Handle special characters properly\n",
        "    },\n",
        "    add_provenance_columns=True,  # Add tracking columns\n",
        "    union_by_name=True            # Handle schema drift across months\n",
        ")\n",
        "\n",
        "# Cache the DataFrame for better performance\n",
        "df.cache()\n",
        "\n",
        "row_count = df.count()\n",
        "print(f\"âœ“ Loaded {row_count:,} rows\")\n",
        "print(f\"âœ“ Columns: {len(df.columns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "woul"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display schema\n",
        "print(\"DataFrame Schema:\")\n",
        "print(\"=\" * 80)\n",
        "df.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show sample data\n",
        "print(\"\\nSample Data (first 10 rows):\")\n",
        "print(\"=\" * 80)\n",
        "display(df.limit(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check provenance columns\n",
        "print(\"Data Provenance:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "provenance_df = df.select(\n",
        "    \"production_month\",\n",
        "    \"file_updated_ts\"\n",
        ").distinct().orderBy(\"production_month\")\n",
        "\n",
        "display(provenance_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Data Quality Checks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count records by production month\n",
        "print(\"Records by Production Month:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "monthly_counts = df.groupBy(\"production_month\") \\\n",
        "    .agg(F.count(\"*\").alias(\"record_count\")) \\\n",
        "    .orderBy(\"production_month\")\n",
        "\n",
        "display(monthly_counts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Write to Delta Table\n",
        "\n",
        "Write the data to a Delta table for further analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define catalog, schema, and table names\n",
        "catalog_name = \"main\"  # or your catalog name\n",
        "schema_name = \"petrinex\"  # or your schema name\n",
        "table_name = \"volumetrics_raw\"\n",
        "\n",
        "full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
        "\n",
        "# Create schema if it doesn't exist\n",
        "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema_name}\")\n",
        "print(f\"âœ“ Schema {catalog_name}.{schema_name} ready\")\n",
        "\n",
        "# Add year and month columns for partitioning\n",
        "df_final = df.withColumn(\n",
        "    \"data_loaded_at\",\n",
        "    F.current_timestamp()\n",
        ").withColumn(\n",
        "    \"year\",\n",
        "    F.substring(F.col(\"production_month\"), 1, 4)\n",
        ").withColumn(\n",
        "    \"month\",\n",
        "    F.substring(F.col(\"production_month\"), 6, 2)\n",
        ")\n",
        "\n",
        "# Write as Delta table with partitioning\n",
        "(\n",
        "    df_final.write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"overwrite\")  # Use \"append\" for incremental loads\n",
        "    .partitionBy(\"year\", \"month\")\n",
        "    .option(\"overwriteSchema\", \"true\")\n",
        "    .saveAsTable(full_table_name)\n",
        ")\n",
        "\n",
        "print(f\"âœ“ Data written to {full_table_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Verify and Query the Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read back from the table\n",
        "result_df = spark.table(full_table_name)\n",
        "\n",
        "print(f\"Table: {full_table_name}\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Total rows: {result_df.count():,}\")\n",
        "print(f\"Partitions: year, month\")\n",
        "\n",
        "# Query the table\n",
        "query_result = spark.sql(f\"\"\"\n",
        "    SELECT \n",
        "        production_month,\n",
        "        COUNT(*) as record_count,\n",
        "        MIN(data_loaded_at) as loaded_at\n",
        "    FROM {full_table_name}\n",
        "    GROUP BY production_month\n",
        "    ORDER BY production_month DESC\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nData Summary by Production Month:\")\n",
        "display(query_result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "âœ… **Completed Steps:**\n",
        "1. Installed petrinex package\n",
        "2. Listed available files from Petrinex\n",
        "3. Loaded data using UC-friendly pandas method (no ANY FILE privilege needed)\n",
        "4. Performed data quality checks\n",
        "5. Wrote to Delta table with year/month partitioning\n",
        "6. Verified the results\n",
        "\n",
        "**Next Steps:**\n",
        "- Schedule this notebook as a job for regular updates\n",
        "- Implement incremental loading based on `file_updated_ts`\n",
        "- Add data validation and alerting\n",
        "- Create downstream analytics tables\n",
        "- Build dashboards and reports\n",
        "\n",
        "**Key Features Used:**\n",
        "- âœ… Unity Catalog compatible (pandas-based read)\n",
        "- âœ… Automatic schema alignment across months\n",
        "- âœ… Provenance tracking (source files, update dates)\n",
        "- âœ… Delta table with partitioning for performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup: Unpersist cached DataFrames\n",
        "df.unpersist()\n",
        "\n",
        "print(\"âœ… Notebook execution complete!\")\n",
        "print(f\"ðŸ“Š Data available at: {full_table_name}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
