{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Petrinex Volumetrics - Load and Display\n",
        "\n",
        "Load Alberta volumetric data from Petrinex into Spark DataFrames.\n",
        "\n",
        "**Features:**\n",
        "- ✅ Unity Catalog compatible (no ANY FILE privilege needed)\n",
        "- ✅ Direct repo import (no pip install needed)\n",
        "- ✅ Auto ZIP extraction (handles nested ZIPs)\n",
        "- ✅ Memory efficient (incremental union + checkpointing)\n",
        "- ✅ Robust error handling (skips missing files)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup - Import from Repo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os\n",
        "\n",
        "# Add repo to Python path\n",
        "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
        "sys.path.insert(0, os.path.dirname(notebook_path))\n",
        "\n",
        "from petrinex import PetrinexVolumetricsClient\n",
        "from pyspark.sql import functions as F\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Initialize client\n",
        "client = PetrinexVolumetricsClient(spark=spark, jurisdiction=\"AB\", file_format=\"CSV\")\n",
        "print(\"✓ Ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## List Available Files (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check files updated in last 30 days\n",
        "cutoff = (datetime.now() - timedelta(days=30)).strftime(\"%Y-%m-%d\")\n",
        "files = client.list_updated_after(cutoff)\n",
        "\n",
        "print(f\"Found {len(files)} files updated after {cutoff}\\n\")\n",
        "for f in files[:10]:\n",
        "    print(f\"{f.production_month} | {f.updated_ts}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data\n",
        "\n",
        "**Memory Efficient:** Unions incrementally as files load + checkpoints every 10 files\n",
        "\n",
        "**Progress:** Shows real-time loading status for each file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data - will show progress for each file\n",
        "df = client.read_updated_after_as_spark_df_via_pandas(\n",
        "    \"2025-12-01\",  # Change date as needed\n",
        "    pandas_read_kwargs={\"dtype\": str, \"encoding\": \"latin1\"}\n",
        ")\n",
        "\n",
        "# Cache the final result\n",
        "df.cache()\n",
        "row_count = df.count()\n",
        "print(f\"\\n✅ Final DataFrame: {row_count:,} rows × {len(df.columns)} columns\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Display Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show schema\n",
        "df.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show sample data\n",
        "display(df.limit(100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Records by production month\n",
        "display(\n",
        "    df.groupBy(\"production_month\")\n",
        "    .agg(F.count(\"*\").alias(\"records\"))\n",
        "    .orderBy(\"production_month\")\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Save to Delta\n",
        "\n",
        "Uncomment to persist data to a Delta table:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to save:\n",
        "# table_name = \"main.petrinex.volumetrics_raw\"\n",
        "# \n",
        "# df_with_parts = df.withColumn(\"year\", F.substring(\"production_month\", 1, 4)) \\\n",
        "#                   .withColumn(\"month\", F.substring(\"production_month\", 6, 2))\n",
        "# \n",
        "# df_with_parts.write.format(\"delta\") \\\n",
        "#     .mode(\"overwrite\") \\\n",
        "#     .partitionBy(\"year\", \"month\") \\\n",
        "#     .saveAsTable(table_name)\n",
        "# \n",
        "# print(f\"✓ Saved to {table_name}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
