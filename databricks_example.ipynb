{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Petrinex Volumetrics Data - Databricks Example\n",
        "\n",
        "This notebook demonstrates how to use the `petrinex` Python package to:\n",
        "1. Fetch Alberta volumetric data from Petrinex\n",
        "2. Load it into Spark DataFrames\n",
        "3. Explore and display the data\n",
        "4. Perform basic analysis\n",
        "\n",
        "**âœ¨ Features:**\n",
        "- âœ… **Unity Catalog Compatible** - No `ANY FILE` privilege required\n",
        "- âœ… **Direct import from repo** - No pip install needed\n",
        "- âœ… **Read-only workflow** - Just load and display (save to Delta is optional)\n",
        "- âœ… **Production ready** - Handles schema drift, encoding, and provenance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup - Import from Repo Directory\n",
        "\n",
        "This notebook imports directly from the Databricks Repo directory.\n",
        "\n",
        "**Requirements:**\n",
        "- This notebook must be in the same repo directory as the `petrinex/` package\n",
        "- Works with Databricks Repos integration (Git sync)\n",
        "- No pip installation needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import directly from Databricks Repo directory\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Get the current notebook's directory (should be in /Workspace/Repos/...)\n",
        "notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
        "repo_root = os.path.dirname(notebook_path)\n",
        "\n",
        "# Add repo root to Python path so we can import petrinex package\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.insert(0, repo_root)\n",
        "\n",
        "print(f\"âœ“ Added repo directory to Python path: {repo_root}\")\n",
        "print(f\"âœ“ Ready to import petrinex package\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initialize Client\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from petrinex import PetrinexVolumetricsClient\n",
        "from pyspark.sql import functions as F\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Initialize the Petrinex client\n",
        "client = PetrinexVolumetricsClient(\n",
        "    spark=spark,\n",
        "    jurisdiction=\"AB\",      # Alberta\n",
        "    file_format=\"CSV\"\n",
        ")\n",
        "\n",
        "print(\"âœ“ Petrinex client initialized successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Explore Available Files\n",
        "\n",
        "First, let's see what files have been updated recently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check what files have been updated in the last 30 days\n",
        "cutoff_date = (datetime.now() - timedelta(days=30)).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "files = client.list_updated_after(cutoff_date)\n",
        "\n",
        "print(f\"Found {len(files)} file(s) updated after {cutoff_date}\\n\")\n",
        "print(\"Production Month | Updated Date        | URL\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "for f in files[:10]:  # Show first 10\n",
        "    print(f\"{f.production_month:15} | {str(f.updated_ts):19} | {f.url}\")\n",
        "\n",
        "if len(files) > 10:\n",
        "    print(f\"\\n... and {len(files) - 10} more files\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Data (Recommended for Unity Catalog)\n",
        "\n",
        "This method downloads files on the driver and avoids Spark file permission issues.\n",
        "\n",
        "**Best Practices:**\n",
        "- Use `dtype=str` to avoid mixed-type column issues\n",
        "- Use `encoding=\"latin1\"` for special characters\n",
        "- Data is automatically unioned across months with schema alignment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the cutoff date (e.g., load data updated in 2026)\n",
        "updated_after = \"2026-01-01\"\n",
        "\n",
        "print(f\"Loading data updated after {updated_after}...\")\n",
        "print(\"This may take a few minutes depending on the number of files...\\n\")\n",
        "\n",
        "# Read data using the pandas-based method (UC-friendly)\n",
        "df = client.read_updated_after_as_spark_df_via_pandas(\n",
        "    updated_after,\n",
        "    pandas_read_kwargs={\n",
        "        \"dtype\": str,           # Force all columns to string (avoid mixed types)\n",
        "        \"encoding\": \"latin1\"    # Handle special characters properly\n",
        "    },\n",
        "    add_provenance_columns=True,  # Add tracking columns\n",
        "    union_by_name=True            # Handle schema drift across months\n",
        ")\n",
        "\n",
        "# Cache the DataFrame for better performance\n",
        "df.cache()\n",
        "\n",
        "row_count = df.count()\n",
        "print(f\"âœ“ Loaded {row_count:,} rows\")\n",
        "print(f\"âœ“ Columns: {len(df.columns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Explore the Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display schema\n",
        "print(\"DataFrame Schema:\")\n",
        "print(\"=\" * 80)\n",
        "df.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show sample data\n",
        "print(\"\\nSample Data (first 10 rows):\")\n",
        "print(\"=\" * 80)\n",
        "display(df.limit(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check provenance columns\n",
        "print(\"Data Provenance:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "provenance_df = df.select(\n",
        "    \"production_month\",\n",
        "    \"file_updated_ts\"\n",
        ").distinct().orderBy(\"production_month\")\n",
        "\n",
        "display(provenance_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Data Quality Checks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count records by production month\n",
        "print(\"Records by Production Month:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "monthly_counts = df.groupBy(\"production_month\") \\\n",
        "    .agg(F.count(\"*\").alias(\"record_count\")) \\\n",
        "    .orderBy(\"production_month\")\n",
        "\n",
        "display(monthly_counts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Additional Analysis (Optional)\n",
        "\n",
        "You can perform additional analysis, transformations, or save to Delta tables as needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Show column statistics\n",
        "print(\"Column Statistics:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Show distinct value counts for key columns\n",
        "key_columns = [\"OperatorBAID\", \"ProductionMonth\", \"ReportingFacilityType\"]\n",
        "\n",
        "for col in key_columns:\n",
        "    if col in df.columns:\n",
        "        distinct_count = df.select(col).distinct().count()\n",
        "        print(f\"{col:30} : {distinct_count:,} distinct values\")\n",
        "\n",
        "# Example: Filter and analyze specific data\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Sample Analysis: Records by Facility Type\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if \"ReportingFacilityType\" in df.columns:\n",
        "    facility_summary = df.groupBy(\"ReportingFacilityType\") \\\n",
        "        .agg(F.count(\"*\").alias(\"record_count\")) \\\n",
        "        .orderBy(F.desc(\"record_count\"))\n",
        "    \n",
        "    display(facility_summary.limit(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup: Unpersist cached DataFrames\n",
        "df.unpersist()\n",
        "\n",
        "print(\"âœ… Notebook execution complete!\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nðŸ“Š Data loaded and displayed successfully\")\n",
        "print(f\"   Total rows: {df.count():,}\")\n",
        "print(f\"   Columns: {len(df.columns)}\")\n",
        "print(f\"\\nðŸ’¡ Next Steps:\")\n",
        "print(\"   - Save to Delta table if needed\")\n",
        "print(\"   - Create visualizations\")\n",
        "print(\"   - Export to other formats\")\n",
        "print(\"   - Perform additional analysis\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Optional: Save to Delta Table\n",
        "\n",
        "If you need to persist the data, uncomment and run the code below:\n",
        "\n",
        "```python\n",
        "# Define your target table\n",
        "catalog_name = \"main\"\n",
        "schema_name = \"petrinex\"\n",
        "table_name = \"volumetrics_raw\"\n",
        "full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
        "\n",
        "# Create schema if needed\n",
        "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema_name}\")\n",
        "\n",
        "# Add partitioning columns\n",
        "df_to_save = df.withColumn(\n",
        "    \"year\", F.substring(F.col(\"production_month\"), 1, 4)\n",
        ").withColumn(\n",
        "    \"month\", F.substring(F.col(\"production_month\"), 6, 2)\n",
        ")\n",
        "\n",
        "# Write to Delta table\n",
        "df_to_save.write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"year\", \"month\") \\\n",
        "    .option(\"overwriteSchema\", \"true\") \\\n",
        "    .saveAsTable(full_table_name)\n",
        "\n",
        "print(f\"âœ“ Data saved to {full_table_name}\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell is intentionally empty - you can add your own code here\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
